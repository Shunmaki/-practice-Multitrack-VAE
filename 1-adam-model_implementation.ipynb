{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf8b8e3",
   "metadata": {},
   "source": [
    "# Multitrack MusicVAE\n",
    "\n",
    "The folowing shows a graphical representation of the model.\n",
    "\n",
    "<img src=\"data/multitrack-music-vae.png\" width=400>\n",
    "\n",
    "\n",
    "The model uses a variational autoencoder. Both the encoder and the decoder have a hierarchical architecture with gated recurrent units (GRU).\n",
    "\n",
    "The encoder takes as input a list of musical sequences encoded in MIDI-like format, to produce one embedding vector. Each musical sequence correspond to one track (piano, saxophone, etc.). Each sequence is processed separately by a shared bi-directional GRU, which produces one embedding vector per track. These track embeddings are processed by a different bi-directional GRU which produces an embedding vector for the whole input.\n",
    "\n",
    "Following the setup of a variational autoencoder, the output of the encoder is fed to two linear layers, to respectively produce a mean vector and a log-variance vector, which are used to parametrize the distribution for sampling the latent vector used for decoding.\n",
    "\n",
    "The decoder takes as input one latent vector and produces a list of musical sequences. The latent vector is fed to a decoder GRU, called the /conductor/, which outputs one embedding for each track to decode. Each track embedding is then fed to a shared decoder GRU which produces a sequnce of event embeddings. Event embeddings are processed by a final linear layer with softmax activation, to calculate the probability distribution over the events in the MIDI-like representation.\n",
    "\n",
    "Links to [long paper](https://arxiv.org/pdf/1806.00195.pdf), [short paper](https://nips2018creativity.github.io/doc/Learning_a_Latent_Space_of_Multitrack_Measures.pdf), [poster](https://colinraffel.com/posters/neurips2018learning.pdf), and the [official implementation in Tensorflow](https://github.com/magenta/magenta/blob/be6558f1a06984faff6d6949234f5fe9ad0ffdb5/magenta/models/music_vae/lstm_models.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdf324",
   "metadata": {},
   "source": [
    "# The encoder\n",
    "The encoder takes as input a tensor of shape (batch_size, n_tracks, seq_len, vocab_size), where\n",
    "- batch_size is the size of batch.\n",
    "- n_tracks is the number of tracks.\n",
    "- seq_len is the length of a sequence.\n",
    "- vocab_size is the size of the vocabulary of events in the MIDI-like format.\n",
    "\n",
    "まず初めに、dummy valuesで通せる様にする。\n",
    "\n",
    "encoder の返り値のshapeは (batch_size, hidden_dim), where `hidden_dim` は潜在空間Zのembedding dimensionであること。\n",
    "とりあえず、hidden_dimは512でやってみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "biblical-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_tracks = 3\n",
    "seq_len = 10\n",
    "vocab_size = 342\n",
    "\n",
    "# Here, we create some dummy data to test the model\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.zeros((batch_size, n_tracks, seq_len, vocab_size)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "improved-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultitrackEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, num_layers):\n",
    "        \"\"\" This initializes the encoder \"\"\"\n",
    "        super(MultitrackEncoder,self).__init__()\n",
    "        self.track_rnn = nn.LSTM(input_size, hidden_dim, batch_first=True, num_layers=num_layers, bidirectional=True, dropout=0.6)\n",
    "        self.score_rnn = nn.LSTM(hidden_dim * 2, hidden_dim, batch_first=True, num_layers=num_layers, bidirectional=True, dropout=0.6)\n",
    "        self.input_size = input_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self,x, h0=None, c0=None):\n",
    "        batch_size, n_tracks, seq_len, input_size  = x.shape\n",
    "        x = x.view(batch_size * n_tracks, seq_len, input_size)\n",
    "        if h0 is None and c0 is None:\n",
    "            h0, c0 = self.init_hidden(batch_size * n_tracks)\n",
    "\n",
    "        _, (h, _) = self.track_rnn(x, (h0, c0))\n",
    "\n",
    "        h = h.view(self.num_layers, 2, batch_size, -1)  # 2 for forward/backward\n",
    "        h = h[-1]\n",
    "        h = torch.cat([h[0], h[1]], dim=1)\n",
    "        h = h.view(batch_size, n_tracks, -1)\n",
    "\n",
    "        h0, c0 = self.init_hidden(batch_size)\n",
    "        _, (h, _) = self.score_rnn(h, (h0, c0))\n",
    "\n",
    "        h = h.view(self.num_layers, 2, batch_size, -1)\n",
    "        h = h[-1]\n",
    "        h = torch.cat([h[0], h[1]], dim=1)\n",
    "        h = h.view(batch_size, -1)\n",
    "\n",
    "        return h\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Bidirectional lstm so num_layers*2\n",
    "        return (torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim, dtype=torch.float, device=device),\n",
    "                torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim, dtype=torch.float, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a75d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = {\n",
    "    \"input_size\": vocab_size,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"num_layers\": 3\n",
    "}\n",
    "\n",
    "encoder = MultitrackEncoder(**encoder_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "129eec9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.8038e-03, -6.2736e-03,  4.3171e-05,  ..., -2.4107e-02,\n",
       "           2.2319e-02, -1.3294e-02],\n",
       "         [-2.5326e-03, -3.9660e-03,  1.0518e-02,  ..., -2.0999e-02,\n",
       "           1.7384e-02, -1.0262e-02],\n",
       "         [-4.2294e-03, -1.4460e-02,  5.9571e-03,  ..., -2.2872e-02,\n",
       "           1.3252e-02, -1.1259e-02],\n",
       "         [-1.7087e-03, -1.0652e-02,  1.1336e-02,  ..., -2.5888e-02,\n",
       "           1.3819e-02, -1.9869e-02]], grad_fn=<ViewBackward0>),\n",
       " torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = encoder(x)\n",
    "hidden, hidden.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54d5c4",
   "metadata": {},
   "source": [
    "# Latent space sampling\n",
    "In this step, the output of the encoder is used to parametrize a gaussian distribution for sampling a latent vector, which will be used by the decoder for recovering the musical score.\n",
    "\n",
    "We compute the mu and log-variance vectors with a linear layer, and we then output sample a latent vector using the reparametrization trick.\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0, 1) \\\\\n",
    "z = \\epsilon \\cdot \\text{stddev} + \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b35a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3365,  1.4243,  1.0314,  ..., -0.2053,  0.3318,  0.1156],\n",
       "         [-0.4156,  0.7189,  0.4965,  ..., -0.1918,  0.6623,  0.9574],\n",
       "         [ 1.3969, -1.8929,  1.6019,  ..., -1.0083, -1.5105, -1.2558],\n",
       "         [ 1.0896,  0.2108, -0.8237,  ...,  0.0660, -0.2607, -1.0459]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([4, 256]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "# from torch.nn import functional as F\n",
    "import random\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "hidden_to_mu = nn.Linear(2 * encoder_config[\"hidden_dim\"], latent_dim).to(device)\n",
    "hidden_to_sig = nn.Linear(2 * encoder_config[\"hidden_dim\"], latent_dim).to(device)\n",
    "mu = hidden_to_mu(hidden)\n",
    "sigma = hidden_to_sig(hidden).exp_()\n",
    "latent_distribution = Normal(mu, sigma)\n",
    "latent = latent_distribution.rsample() # Reparametrization using rsample method in PyTorch\n",
    "latent, latent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e563bac",
   "metadata": {},
   "source": [
    "# The decoder (TODO)\n",
    "decoderは潜在ベクトルを入力とする.\n",
    "潜在ベクトルのshapeは (batch_size, latent_dim). \n",
    "出力は MIDI-likeの確率値(tensor)となり、その形は`(batch_size, n_tracks, seq_len, vocab_size)`.\n",
    "`n_tracks`は簡単に言うと楽器の数\n",
    "\n",
    "デコーダーは2つのモデルが存在\n",
    "- トレーニングモード: 各time-stepのGRUには、本物のデータを入力とする。\n",
    "- 推論モード: 一個前のtime-sequeceの出力を入力年、自己回帰的に出力を求めてゆく\n",
    "\n",
    "\n",
    "まずは、inferenceモードを実装する。inferenceモードができればtrainingモードは簡単に実装できる(多分)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21072e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MultitrackDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, latent_dim, conductor_hidden_dim, conductor_output_dim,\n",
    "                 decoder_hidden_dim, num_layers, seq_len, n_tracks):\n",
    "        super(MultitrackDecoder, self).__init__()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        conductor_input_dim = latent_dim\n",
    "        self.conductor_input = nn.Parameter(torch.rand(conductor_input_dim))\n",
    "        self.conductor_hidden_linear = nn.Linear(conductor_input_dim, conductor_hidden_dim * num_layers)\n",
    "        self.conductor_rnn = nn.LSTM(latent_dim, conductor_hidden_dim, batch_first=True, num_layers=num_layers, bidirectional=False, dropout=0.6)\n",
    "        assert conductor_output_dim is None\n",
    "        conductor_output_dim = conductor_hidden_dim\n",
    "        self.conductor_output_linear = nn.Linear(conductor_hidden_dim, conductor_output_dim)\n",
    "        \n",
    "        # self.decoder_hidden_linear = nn.Linear(conductor_output_dim, decoder_hidden_dim * num_layers)\n",
    "        self.decoder_hidden_linear = nn.Linear(latent_dim, decoder_hidden_dim * num_layers)\n",
    "        # self.decoder_rnn = nn.LSTM(conductor_output_dim + input_size, decoder_hidden_dim, batch_first=True, num_layers=num_layers, bidirectional=False, dropout=0.6)\n",
    "        self.decoder_rnn = nn.LSTM(latent_dim + input_size, decoder_hidden_dim, batch_first=True, num_layers=num_layers, bidirectional=False, dropout=0.6)\n",
    "        self.decoder_output = nn.Linear(decoder_hidden_dim, input_size)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.conductor_hidden_dim = conductor_hidden_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_len\n",
    "        self.n_tracks = n_tracks\n",
    "        self.teacher_forcing_ratio = 0.5\n",
    "\n",
    "    def sample(self, x, method=\"argmax\"):\n",
    "        idx = x.max(1)[1]\n",
    "        x = torch.zeros_like(x)\n",
    "        arange = torch.arange(x.size(0)).long()\n",
    "        if torch.cuda.is_available():\n",
    "            arange = arange.cuda()\n",
    "        x[arange, idx] = 1\n",
    "        return x\n",
    "\n",
    "    def forward(self, latent, target=None, seq_len=0., teacher_forcing=True):\n",
    "        batch_size = latent.shape[0]\n",
    "        device = latent.device\n",
    "        conductor_hidden = self.conductor_hidden_linear(self.conductor_input.repeat(batch_size, 1))\n",
    "        conductor_hidden = conductor_hidden.view(batch_size, self.num_layers, -1).transpose(0, 1).contiguous()\n",
    "        conductor_cell = conductor_hidden\n",
    "        tracks_sequences = []\n",
    "\n",
    "        for track in range(self.n_tracks):\n",
    "            _, (conductor_hidden, conductor_cell) = self.conductor_rnn(latent.unsqueeze(1),\n",
    "                                                                       (conductor_hidden, conductor_cell))\n",
    "            track_embedding = conductor_hidden[-1]\n",
    "            track_embedding = self.conductor_output_linear(track_embedding)\n",
    "\n",
    "            decoder_hidden = self.decoder_hidden_linear(track_embedding)\n",
    "            decoder_hidden = decoder_hidden.view(batch_size, self.num_layers, -1).transpose(0, 1).contiguous()\n",
    "            decoder_cell = decoder_hidden\n",
    "            previous_event = torch.zeros((batch_size, self.input_size)).to(device)\n",
    "            previous_event[:, -1] = 1.         # Set as SOS token in one-hot representation\n",
    "            track_sequence = []\n",
    "            for event in range(seq_len):\n",
    "                previous_event = torch.cat([previous_event, track_embedding], 1).unsqueeze(1)\n",
    "                _, (decoder_hidden, decoder_cell) = self.decoder_rnn(previous_event, (decoder_hidden, decoder_cell))\n",
    "                event_probabilities = F.log_softmax(self.decoder_output(decoder_hidden[-1]), 1)\n",
    "                track_sequence.append(event_probabilities)\n",
    "                if self.training and teacher_forcing:\n",
    "                    assert target is not None, \"How am I supposed to teacher force without the target data??\"\n",
    "                    p = torch.rand(1).item()\n",
    "                    if p < self.teacher_forcing_ratio:\n",
    "                        previous_event = target[:, track, event, :]\n",
    "                    else:\n",
    "                        previous_event = self.sample(event_probabilities)\n",
    "                else:\n",
    "                    previous_event = self.sample(event_probabilities)\n",
    "            tracks_sequences.append(torch.stack(track_sequence, dim=1))\n",
    "\n",
    "        return torch.stack(tracks_sequences, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e9eed13",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28150/2626550239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"n_tracks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_tracks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultitrackDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdecoder_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_28150/440923741.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, latent_dim, conductor_hidden_dim, conductor_output_dim, decoder_hidden_dim, num_layers, seq_len, n_tracks)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconductor_hidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconductor_input_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconductor_hidden_dim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconductor_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconductor_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mconductor_output_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mconductor_output_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconductor_hidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconductor_output_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconductor_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconductor_output_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "decoder_config = {\n",
    "    \"input_size\": vocab_size,\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"conductor_hidden_dim\": 256,\n",
    "    \"conductor_output_dim\": 256,\n",
    "    \"decoder_hidden_dim\": 256,\n",
    "    \"num_layers\":3,\n",
    "    \"seq_len\": seq_len,\n",
    "    \"n_tracks\": n_tracks\n",
    "}\n",
    "decoder = MultitrackDecoder(**decoder_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
